{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pymahjong"
      ],
      "metadata": {
        "id": "j8ZJ-wr4OVRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C13J1W5ZOOoc"
      },
      "outputs": [],
      "source": [
        "import pymahjong\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TERMINAL_HONOR_INDICES = set([\n",
        "    0, 8, 9, 17, 18, 26, 27, 28, 29, 30, 31, 32, 33\n",
        "])\n",
        "\n",
        "WIN_INDICES = set([42,43])\n",
        "\n",
        "class THW():\n",
        "\n",
        "  def select_action(self, obs_np, valid_actions):\n",
        "    win_actions = [a for a in valid_actions if a in WIN_INDICES]\n",
        "    if win_actions:\n",
        "        return np.random.choice(win_actions)\n",
        "    terminal_honor_discards = [a for a in valid_actions if a in TERMINAL_HONOR_INDICES]\n",
        "\n",
        "    if terminal_honor_discards:\n",
        "        return np.random.choice(terminal_honor_discards)\n",
        "    else:\n",
        "        return np.random.choice(valid_actions)\n",
        "\n",
        "class TH():\n",
        "\n",
        "  def select_action(self, obs_np, valid_actions):\n",
        "    terminal_honor_discards = [a for a in valid_actions if a in TERMINAL_HONOR_INDICES]\n",
        "    if terminal_honor_discards:\n",
        "        return np.random.choice(terminal_honor_discards)\n",
        "    else:\n",
        "        return np.random.choice(valid_actions)\n",
        "\n",
        "class RAND():\n",
        "  def select_action(self, obs_np, valid_actions):\n",
        "    return np.random.choice(valid_actions)\n"
      ],
      "metadata": {
        "id": "dICRb_gZghsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_game(agents, num_rounds=1000):\n",
        "  reward_sum = 0\n",
        "  for _ in range(num_rounds):\n",
        "    env = pymahjong.MahjongEnv()\n",
        "    obs = env.reset()\n",
        "    while True:\n",
        "        curr_pid = env.get_curr_player_id()\n",
        "        valid_actions = env.get_valid_actions()  # e.g., [0, 3, 4, 20, 21]\n",
        "        executor_obs = env.get_obs(curr_pid)\n",
        "        a = agents[curr_pid].select_action(executor_obs, valid_actions)\n",
        "\n",
        "        env.step(curr_pid, a)\n",
        "\n",
        "\n",
        "        # print(executor_obs)\n",
        "        # oracle_obs = env.get_oracle_obs(curr_pid)\n",
        "        # full_obs = concat((executor_obs, oracle_obs), axis=0)\n",
        "\n",
        "        if env.is_over():\n",
        "            payoffs = env.get_payoffs() # payoffs = [p0, p1, p2, p3]\n",
        "            reward_sum += payoffs[0]\n",
        "            break\n",
        "  print(\"total payoff = {} after {} rounds\".format(reward_sum, num_rounds))"
      ],
      "metadata": {
        "id": "jLhnOy1vekuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"terminal / honor discard agent\")\n",
        "run_game(agents=[TH()] * 4, num_rounds=1000)\n",
        "print(\"terminal / honor / win discard agent\")\n",
        "run_game(agents=[THW()] * 4, num_rounds=1000)\n",
        "print(\"random agent\")\n",
        "run_game(agents=[RAND()] * 4, num_rounds=1000)"
      ],
      "metadata": {
        "id": "zI_mFL4Ekz2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training a cnn"
      ],
      "metadata": {
        "id": "LCVCe1m3IG8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MahjongCNNPolicy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 93 * 34, 512)\n",
        "        self.fc2 = nn.Linear(512, 47)\n",
        "\n",
        "    def forward(self, x):  # x: [batch_size, 1, 93, 34]\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)  # logits"
      ],
      "metadata": {
        "id": "Zk4tnK-CIIyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNAgent:\n",
        "    def __init__(self, model, device=\"cpu\"):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "\n",
        "    def select_action(self, obs_np, valid_actions, training=True):\n",
        "        # Convert 93Ã—34 obs to torch tensor [1, 1, 93, 34]\n",
        "        obs_tensor = torch.tensor(obs_np, dtype=torch.float32, device=self.device).unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "        if not training:\n",
        "          with torch.no_grad():\n",
        "            logits = self.model(obs_tensor)[0]\n",
        "        else:\n",
        "            logits = self.model(obs_tensor)[0]  # no torch.no_grad() here!\n",
        "\n",
        "        valid_actions = [a for a in valid_actions if 0 <= a < 47]\n",
        "\n",
        "\n",
        "        logits = logits[0]  # [47]\n",
        "\n",
        "        # Mask invalid actions\n",
        "        mask = torch.full((47,), float('-inf'), device=self.device)\n",
        "        mask[valid_actions] = 0  # allow valid actions only\n",
        "\n",
        "        masked_logits = logits + mask\n",
        "        probs = F.softmax(masked_logits, dim=0)\n",
        "\n",
        "        dist = torch.distributions.Categorical(probs)\n",
        "        action = dist.sample()\n",
        "\n",
        "        return action.item(), dist.log_prob(action)"
      ],
      "metadata": {
        "id": "l6okhc1zI0in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_cnn_agent_w_randoms(num_games=1000, learning_rate=1e-4):\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = MahjongCNNPolicy()\n",
        "    agent = CNNAgent(model, device=device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    all_log_probs = []\n",
        "    all_rewards = []\n",
        "\n",
        "    for episode in range(num_games):\n",
        "        env = pymahjong.MahjongEnv()\n",
        "        obs = env.reset()\n",
        "\n",
        "        log_probs = []\n",
        "        rewards = []\n",
        "\n",
        "        while True:\n",
        "            pid = env.get_curr_player_id()\n",
        "            valid_actions = env.get_valid_actions()\n",
        "            executor_obs = env.get_obs(pid)\n",
        "\n",
        "            if pid == 0: #cnn agent\n",
        "                a, log_prob = agent.select_action(executor_obs, valid_actions, training=True)\n",
        "                print(f\"cnn turn action: {a}\")\n",
        "                log_probs.append(log_prob)\n",
        "            else: # 3 other random agents\n",
        "                a = np.random.choice(valid_actions)\n",
        "\n",
        "            env.step(pid, a)\n",
        "\n",
        "            if env.is_over():\n",
        "              payoffs = env.get_payoffs() # payoffs = [p0, p1, p2, p3]\n",
        "              break\n",
        "\n",
        "        reward = payoffs[0]\n",
        "\n",
        "\n",
        "        # Save logs for training\n",
        "        all_log_probs.append(log_probs)\n",
        "        all_rewards.append(reward)\n",
        "\n",
        "        # Training step (REINFORCE)\n",
        "        optimizer.zero_grad()\n",
        "        loss = 0\n",
        "        for log_prob in log_probs:\n",
        "            loss += -log_prob * reward\n",
        "        print(f\"raw reward {reward}. raw log prob[0]: {log_probs[0].item()}. loss: {loss}\")\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        print(f\"Episode [{episode}] reward: {reward:.2f}, loss: {loss.item():.2f}\")\n",
        "\n",
        "        # Save the model\n",
        "    torch.save(model.state_dict(), \"trained_cnn_rand_model.pth\")\n",
        "    print(\"Training complete and model saved.\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "fW5QLpNxJDJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_cnn_agent_w_randoms()\n",
        "# run_game(agents=[model, RAND(), RAND(), RAND()], num_rounds=1000)"
      ],
      "metadata": {
        "id": "bVStlGD4Jtdp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
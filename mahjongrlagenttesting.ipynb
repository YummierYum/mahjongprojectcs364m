{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqQq-fTUMdlT",
        "outputId": "9fab4348-cfe0-454b-8873-f52ce709260f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymahjong\n",
            "  Downloading pymahjong-1.0.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_31_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pymahjong) (2.0.2)\n",
            "Requirement already satisfied: gym<=0.26.2 in /usr/local/lib/python3.11/dist-packages (from pymahjong) (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym<=0.26.2->pymahjong) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym<=0.26.2->pymahjong) (0.0.8)\n",
            "Downloading pymahjong-1.0.4-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_31_x86_64.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m413.5/413.5 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymahjong\n",
            "Successfully installed pymahjong-1.0.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pymahjong"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Section 1: Setup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from pymahjong import MahjongEnv\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "OBS_CHANNELS = 93\n",
        "HEIGHT = 34\n",
        "WIDTH = 1\n",
        "NUM_ACTIONS = 136\n"
      ],
      "metadata": {
        "id": "ntM6pBG9MpTZ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_game_state(obs):\n",
        "    \"\"\"\n",
        "    Converts pymahjong (93, 34) obs into CNN input format: (1, 93, 34, 1)\n",
        "    \"\"\"\n",
        "    obs = np.array(obs, dtype=np.float32)  # (93, 34)\n",
        "    return torch.tensor(obs).unsqueeze(0).unsqueeze(-1).to(device)  # (1, 93, 34, 1)\n"
      ],
      "metadata": {
        "id": "gE9RwaS_ecZW"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MahjongCNNBase(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(OBS_CHANNELS, 64, kernel_size=(3, 1), padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=(3, 1), padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        self.output_dim = self._get_output_dim()\n",
        "\n",
        "    def _get_output_dim(self):\n",
        "        dummy = torch.zeros((1, OBS_CHANNELS, HEIGHT, WIDTH))\n",
        "        return self.conv(dummy).shape[1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class DiscreteHead(nn.Module):\n",
        "    def __init__(self, base, output_size):\n",
        "        super().__init__()\n",
        "        self.base = base\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(base.output_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.head(self.base(x))\n"
      ],
      "metadata": {
        "id": "E6dgHvKKee_d"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingBot:\n",
        "    def __init__(self, pid, models, logs):\n",
        "        self.pid = pid\n",
        "        self.models = models\n",
        "        self.logs = logs\n",
        "\n",
        "    def act(self, obs, valid_actions, reward=0.0):\n",
        "        state = encode_game_state(obs)  # shape: (1, 93, 34, 1)\n",
        "        logits = self.models[\"action\"](state)  # shape: (1, N) where N = model output (e.g. 136)\n",
        "        logits = logits.squeeze(0)\n",
        "\n",
        "        # Only use logits for valid actions\n",
        "        valid_logits = logits[valid_actions]\n",
        "        probs = F.softmax(valid_logits, dim=0).detach().cpu().numpy()\n",
        "        action_index = np.random.choice(len(valid_actions), p=probs)\n",
        "        action = valid_actions[action_index]\n",
        "\n",
        "        self.logs.append((state.squeeze(0), action, reward))\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "_zkdNyGbeh70"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pg(model, optimizer, log_data, gamma=1.0):\n",
        "    \"\"\"\n",
        "    Train a policy using REINFORCE loss.\n",
        "    log_data: list of (state, action, reward)\n",
        "    \"\"\"\n",
        "    if not log_data:\n",
        "        return 0.0\n",
        "\n",
        "    states, actions, rewards = zip(*log_data)\n",
        "    states = torch.stack(states).to(device)\n",
        "    actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
        "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
        "\n",
        "    # Use baseline to reduce variance\n",
        "    baseline = rewards.mean()\n",
        "    advantages = rewards - baseline\n",
        "    print(\"\\nüîç DEBUG TRAINING BATCH:\")\n",
        "    print(f\"Rewards: {rewards}\")\n",
        "    print(f\"Baseline: {baseline}\")\n",
        "    print(f\"Advantages: {advantages}\")\n",
        "\n",
        "    for i, (s, a, r) in enumerate(log_data[:5]):\n",
        "        print(f\"Sample {i}: action={a}, reward={r}\")\n",
        "\n",
        "    # Forward\n",
        "    logits = model(states)  # shape: (batch, num_actions)\n",
        "    log_probs = F.log_softmax(logits, dim=1)\n",
        "    selected_log_probs = log_probs[torch.arange(len(actions)), actions]\n",
        "\n",
        "    # REINFORCE loss: maximize reward-weighted log-prob\n",
        "    loss = -torch.mean(advantages * selected_log_probs)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n"
      ],
      "metadata": {
        "id": "OOGnThqhNWNg"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init model + optimizer\n",
        "base = MahjongCNNBase().to(device)\n",
        "models = {\n",
        "    \"action\": DiscreteHead(base, NUM_ACTIONS).to(device)\n",
        "}\n",
        "optimizers = {\n",
        "    name: torch.optim.Adam(models[name].parameters(), lr=1e-4)\n",
        "    for name in models\n",
        "}\n",
        "\n",
        "NUM_EPISODES = 1000\n",
        "SAVE_EVERY = 100\n",
        "\n",
        "for episode in range(1, NUM_EPISODES + 1):\n",
        "    env = MahjongEnv()\n",
        "    env.reset()\n",
        "    logs = []\n",
        "\n",
        "    bots = [\n",
        "        TrainingBot(pid, models, logs if pid == 0 else [])\n",
        "        for pid in range(4)\n",
        "    ]\n",
        "\n",
        "    while not env.is_over():\n",
        "        pid = env.get_curr_player_id()\n",
        "        obs = env.get_obs(pid)\n",
        "        valid = env.get_valid_actions()\n",
        "        action = bots[pid].act(obs, valid)\n",
        "        env.step(pid, action)\n",
        "\n",
        "    # Get final reward for REINFORCE\n",
        "    payoffs = env.get_payoffs()  # [p0, p1, p2, p3]\n",
        "    for i in range(len(logs)):\n",
        "        state, action, _ = logs[i]\n",
        "        logs[i] = (state, action, payoffs[0])\n",
        "\n",
        "    # Train\n",
        "    loss = train_pg(models[\"action\"], optimizers[\"action\"], logs)\n",
        "    print(f\"[Episode {episode}] Reward: {payoffs[0]:.1f} | Loss: {loss:.4f}\")\n",
        "\n",
        "    if episode % SAVE_EVERY == 0:\n",
        "        os.makedirs(f\"checkpoints/ep_{episode}\", exist_ok=True)\n",
        "        torch.save(models[\"action\"].state_dict(), f\"checkpoints/ep_{episode}/action.pt\")\n",
        "        print(f\"‚úì Model saved at ep {episode}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwSHEh2gNbxW",
        "outputId": "e441d0e4-2ec0-4af1-d6d2-c1722996ff95"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "        -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=-1000.0\n",
            "Sample 1: action=40, reward=-1000.0\n",
            "Sample 2: action=9, reward=-1000.0\n",
            "Sample 3: action=41, reward=-1000.0\n",
            "Sample 4: action=29, reward=-1000.0\n",
            "[Episode 566] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=-1000.0\n",
            "Sample 1: action=29, reward=-1000.0\n",
            "Sample 2: action=8, reward=-1000.0\n",
            "Sample 3: action=27, reward=-1000.0\n",
            "Sample 4: action=44, reward=-1000.0\n",
            "[Episode 567] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=27, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=42, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 568] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=26, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=11, reward=0.0\n",
            "[Episode 569] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=24, reward=0.0\n",
            "Sample 3: action=30, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 570] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=24, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 571] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=14, reward=0.0\n",
            "[Episode 572] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=37, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 573] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=30, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 574] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 575] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=26, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 576] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=39, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=37, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 577] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=26, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=14, reward=0.0\n",
            "[Episode 578] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=0, reward=0.0\n",
            "Sample 2: action=43, reward=0.0\n",
            "Sample 3: action=29, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 579] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=27, reward=0.0\n",
            "Sample 2: action=42, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=18, reward=0.0\n",
            "[Episode 580] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=0.0\n",
            "Sample 1: action=38, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 581] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=0, reward=0.0\n",
            "Sample 2: action=41, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=4, reward=0.0\n",
            "[Episode 582] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 583] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=40, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 584] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=38, reward=0.0\n",
            "Sample 2: action=0, reward=0.0\n",
            "Sample 3: action=29, reward=0.0\n",
            "Sample 4: action=12, reward=0.0\n",
            "[Episode 585] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([12000., 12000., 12000., 12000., 12000., 12000., 12000., 12000., 12000.,\n",
            "        12000., 12000., 12000., 12000., 12000., 12000., 12000., 12000., 12000.,\n",
            "        12000., 12000., 12000., 12000., 12000., 12000., 12000., 12000., 12000.,\n",
            "        12000.])\n",
            "Baseline: 12000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=12000.0\n",
            "Sample 1: action=9, reward=12000.0\n",
            "Sample 2: action=28, reward=12000.0\n",
            "Sample 3: action=43, reward=12000.0\n",
            "Sample 4: action=20, reward=12000.0\n",
            "[Episode 586] Reward: 12000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=43, reward=0.0\n",
            "Sample 2: action=13, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 587] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=9, reward=0.0\n",
            "Sample 1: action=37, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=46, reward=0.0\n",
            "Sample 4: action=0, reward=0.0\n",
            "[Episode 588] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=21, reward=-1000.0\n",
            "Sample 1: action=43, reward=-1000.0\n",
            "Sample 2: action=26, reward=-1000.0\n",
            "Sample 3: action=12, reward=-1000.0\n",
            "Sample 4: action=23, reward=-1000.0\n",
            "[Episode 589] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([1500., 1500., 1500., 1500., 1500., 1500., 1500., 1500., 1500., 1500.,\n",
            "        1500., 1500., 1500.])\n",
            "Baseline: 1500.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=1500.0\n",
            "Sample 1: action=21, reward=1500.0\n",
            "Sample 2: action=38, reward=1500.0\n",
            "Sample 3: action=5, reward=1500.0\n",
            "Sample 4: action=9, reward=1500.0\n",
            "[Episode 590] Reward: 1500.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=9, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 591] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=5, reward=0.0\n",
            "Sample 1: action=9, reward=0.0\n",
            "Sample 2: action=38, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 592] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=38, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=9, reward=0.0\n",
            "[Episode 593] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=28, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=38, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=9, reward=0.0\n",
            "[Episode 594] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=21, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 595] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=27, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=38, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=13, reward=0.0\n",
            "[Episode 596] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=27, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=26, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 597] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=42, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 598] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=4, reward=0.0\n",
            "Sample 3: action=44, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 599] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=4, reward=0.0\n",
            "Sample 3: action=26, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 600] Reward: 0.0 | Loss: -0.0000\n",
            "‚úì Model saved at ep 600\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=4, reward=0.0\n",
            "Sample 3: action=3, reward=0.0\n",
            "Sample 4: action=37, reward=0.0\n",
            "[Episode 601] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=4, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=13, reward=0.0\n",
            "[Episode 602] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=3, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 603] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=27, reward=0.0\n",
            "Sample 1: action=17, reward=0.0\n",
            "Sample 2: action=42, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 604] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=27, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=26, reward=0.0\n",
            "Sample 4: action=42, reward=0.0\n",
            "[Episode 605] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=11, reward=0.0\n",
            "Sample 4: action=5, reward=0.0\n",
            "[Episode 606] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=5, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 607] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=27, reward=0.0\n",
            "Sample 4: action=42, reward=0.0\n",
            "[Episode 608] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=27, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=5, reward=0.0\n",
            "Sample 3: action=25, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 609] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=12, reward=0.0\n",
            "Sample 2: action=5, reward=0.0\n",
            "Sample 3: action=26, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 610] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=17, reward=0.0\n",
            "Sample 2: action=35, reward=0.0\n",
            "Sample 3: action=42, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 611] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=28, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=13, reward=0.0\n",
            "[Episode 612] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=18, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=2, reward=0.0\n",
            "[Episode 613] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=32, reward=0.0\n",
            "Sample 4: action=20, reward=0.0\n",
            "[Episode 614] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=33, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=20, reward=0.0\n",
            "[Episode 615] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=20, reward=3000.0\n",
            "Sample 1: action=5, reward=3000.0\n",
            "Sample 2: action=11, reward=3000.0\n",
            "Sample 3: action=53, reward=3000.0\n",
            "Sample 4: action=19, reward=3000.0\n",
            "[Episode 616] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=34, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=27, reward=0.0\n",
            "[Episode 617] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 618] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 619] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=-1000.0\n",
            "Sample 1: action=25, reward=-1000.0\n",
            "Sample 2: action=31, reward=-1000.0\n",
            "Sample 3: action=4, reward=-1000.0\n",
            "Sample 4: action=16, reward=-1000.0\n",
            "[Episode 620] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=4, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=38, reward=0.0\n",
            "[Episode 621] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=4, reward=0.0\n",
            "Sample 2: action=29, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=5, reward=0.0\n",
            "[Episode 622] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=2, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=21, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 623] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=20, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 624] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=4, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 625] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=4, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 626] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=15, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 627] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=26, reward=0.0\n",
            "[Episode 628] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=4, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 629] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=0.0\n",
            "Sample 1: action=10, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 630] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-3900., -3900., -3900., -3900., -3900., -3900., -3900., -3900., -3900.,\n",
            "        -3900., -3900., -3900., -3900., -3900., -3900., -3900., -3900., -3900.,\n",
            "        -3900., -3900., -3900., -3900.])\n",
            "Baseline: -3900.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=-3900.0\n",
            "Sample 1: action=26, reward=-3900.0\n",
            "Sample 2: action=43, reward=-3900.0\n",
            "Sample 3: action=13, reward=-3900.0\n",
            "Sample 4: action=29, reward=-3900.0\n",
            "[Episode 631] Reward: -3900.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=4, reward=0.0\n",
            "Sample 2: action=10, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 632] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=19, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 633] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=19, reward=0.0\n",
            "Sample 2: action=29, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 634] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=20, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 635] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=21, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 636] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=18, reward=0.0\n",
            "Sample 2: action=43, reward=0.0\n",
            "Sample 3: action=1, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 637] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=1, reward=0.0\n",
            "[Episode 638] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=18, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=15, reward=0.0\n",
            "[Episode 639] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=10, reward=0.0\n",
            "Sample 3: action=1, reward=0.0\n",
            "Sample 4: action=3, reward=0.0\n",
            "[Episode 640] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=18, reward=0.0\n",
            "Sample 1: action=21, reward=0.0\n",
            "Sample 2: action=41, reward=0.0\n",
            "Sample 3: action=22, reward=0.0\n",
            "Sample 4: action=15, reward=0.0\n",
            "[Episode 641] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=3, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 642] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=18, reward=0.0\n",
            "[Episode 643] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=4, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 644] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=4, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 645] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=43, reward=0.0\n",
            "Sample 2: action=29, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 646] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=4, reward=0.0\n",
            "[Episode 647] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=4, reward=0.0\n",
            "Sample 3: action=30, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 648] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=28, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 649] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=3, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=29, reward=0.0\n",
            "Sample 3: action=4, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 650] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=43, reward=0.0\n",
            "Sample 2: action=3, reward=0.0\n",
            "Sample 3: action=11, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 651] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=42, reward=0.0\n",
            "Sample 4: action=24, reward=0.0\n",
            "[Episode 652] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=2, reward=0.0\n",
            "Sample 1: action=34, reward=0.0\n",
            "Sample 2: action=10, reward=0.0\n",
            "Sample 3: action=25, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 653] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 654] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=41, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 655] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=41, reward=0.0\n",
            "Sample 2: action=29, reward=0.0\n",
            "Sample 3: action=25, reward=0.0\n",
            "Sample 4: action=38, reward=0.0\n",
            "[Episode 656] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=41, reward=0.0\n",
            "Sample 4: action=13, reward=0.0\n",
            "[Episode 657] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=0.0\n",
            "Sample 1: action=38, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 658] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=34, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=40, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 659] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=40, reward=0.0\n",
            "Sample 3: action=34, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 660] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=28, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 661] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([5800., 5800., 5800., 5800., 5800., 5800., 5800., 5800., 5800., 5800.,\n",
            "        5800., 5800., 5800., 5800., 5800., 5800., 5800., 5800., 5800., 5800.,\n",
            "        5800., 5800., 5800., 5800., 5800., 5800.])\n",
            "Baseline: 5800.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=26, reward=5800.0\n",
            "Sample 1: action=53, reward=5800.0\n",
            "Sample 2: action=16, reward=5800.0\n",
            "Sample 3: action=36, reward=5800.0\n",
            "Sample 4: action=7, reward=5800.0\n",
            "[Episode 662] Reward: 5800.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=3, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=37, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 663] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=21, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 664] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=16, reward=-1000.0\n",
            "Sample 1: action=23, reward=-1000.0\n",
            "Sample 2: action=3, reward=-1000.0\n",
            "Sample 3: action=14, reward=-1000.0\n",
            "Sample 4: action=53, reward=-1000.0\n",
            "[Episode 665] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=3, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=40, reward=0.0\n",
            "Sample 3: action=22, reward=0.0\n",
            "Sample 4: action=14, reward=0.0\n",
            "[Episode 666] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 667] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0.])\n",
            "Sample 0: action=6, reward=3000.0\n",
            "Sample 1: action=28, reward=3000.0\n",
            "Sample 2: action=25, reward=3000.0\n",
            "Sample 3: action=42, reward=3000.0\n",
            "Sample 4: action=26, reward=3000.0\n",
            "[Episode 668] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=33, reward=-1000.0\n",
            "Sample 1: action=18, reward=-1000.0\n",
            "Sample 2: action=31, reward=-1000.0\n",
            "Sample 3: action=15, reward=-1000.0\n",
            "Sample 4: action=9, reward=-1000.0\n",
            "[Episode 669] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=8, reward=0.0\n",
            "Sample 4: action=18, reward=0.0\n",
            "[Episode 670] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 671] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 672] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=33, reward=0.0\n",
            "[Episode 673] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=15, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 674] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=40, reward=0.0\n",
            "[Episode 675] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 676] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=9, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=44, reward=0.0\n",
            "Sample 3: action=8, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 677] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 678] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=2, reward=0.0\n",
            "Sample 1: action=10, reward=0.0\n",
            "Sample 2: action=42, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 679] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=2, reward=-1000.0\n",
            "Sample 1: action=12, reward=-1000.0\n",
            "Sample 2: action=43, reward=-1000.0\n",
            "Sample 3: action=28, reward=-1000.0\n",
            "Sample 4: action=39, reward=-1000.0\n",
            "[Episode 680] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=2, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=9, reward=0.0\n",
            "Sample 3: action=12, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 681] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=3, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=15, reward=0.0\n",
            "[Episode 682] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0.])\n",
            "Sample 0: action=3, reward=3000.0\n",
            "Sample 1: action=39, reward=3000.0\n",
            "Sample 2: action=17, reward=3000.0\n",
            "Sample 3: action=0, reward=3000.0\n",
            "Sample 4: action=2, reward=3000.0\n",
            "[Episode 683] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=3, reward=0.0\n",
            "Sample 1: action=12, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=42, reward=0.0\n",
            "[Episode 684] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=3, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 685] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=37, reward=0.0\n",
            "Sample 2: action=9, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 686] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=20, reward=0.0\n",
            "Sample 2: action=1, reward=0.0\n",
            "Sample 3: action=44, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 687] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=6, reward=-1000.0\n",
            "Sample 1: action=20, reward=-1000.0\n",
            "Sample 2: action=17, reward=-1000.0\n",
            "Sample 3: action=30, reward=-1000.0\n",
            "Sample 4: action=9, reward=-1000.0\n",
            "[Episode 688] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=17, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=21, reward=0.0\n",
            "[Episode 689] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=28, reward=0.0\n",
            "Sample 2: action=41, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 690] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=29, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 691] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=19, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 692] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=19, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=13, reward=0.0\n",
            "[Episode 693] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=18, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=24, reward=0.0\n",
            "[Episode 694] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=40, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=21, reward=0.0\n",
            "[Episode 695] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=26, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=37, reward=0.0\n",
            "Sample 4: action=0, reward=0.0\n",
            "[Episode 696] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=39, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=12, reward=0.0\n",
            "[Episode 697] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=12, reward=0.0\n",
            "Sample 4: action=41, reward=0.0\n",
            "[Episode 698] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=26, reward=0.0\n",
            "Sample 2: action=18, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 699] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=43, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=11, reward=0.0\n",
            "[Episode 700] Reward: 0.0 | Loss: -0.0000\n",
            "‚úì Model saved at ep 700\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=0, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=11, reward=0.0\n",
            "[Episode 701] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=5, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=9, reward=0.0\n",
            "[Episode 702] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=15, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=44, reward=0.0\n",
            "Sample 3: action=30, reward=0.0\n",
            "Sample 4: action=0, reward=0.0\n",
            "[Episode 703] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=9, reward=0.0\n",
            "Sample 2: action=1, reward=0.0\n",
            "Sample 3: action=15, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 704] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=9, reward=-1000.0\n",
            "Sample 1: action=19, reward=-1000.0\n",
            "Sample 2: action=14, reward=-1000.0\n",
            "Sample 3: action=38, reward=-1000.0\n",
            "Sample 4: action=0, reward=-1000.0\n",
            "[Episode 705] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=26, reward=0.0\n",
            "Sample 4: action=39, reward=0.0\n",
            "[Episode 706] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=5, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=9, reward=0.0\n",
            "Sample 3: action=15, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 707] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=5, reward=-1000.0\n",
            "Sample 1: action=53, reward=-1000.0\n",
            "Sample 2: action=30, reward=-1000.0\n",
            "Sample 3: action=31, reward=-1000.0\n",
            "Sample 4: action=36, reward=-1000.0\n",
            "[Episode 708] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=9, reward=0.0\n",
            "[Episode 709] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1300., -1300., -1300., -1300., -1300., -1300., -1300., -1300., -1300.,\n",
            "        -1300., -1300., -1300., -1300., -1300., -1300., -1300., -1300., -1300.,\n",
            "        -1300., -1300., -1300., -1300., -1300., -1300., -1300.])\n",
            "Baseline: -1300.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=3, reward=-1300.0\n",
            "Sample 1: action=21, reward=-1300.0\n",
            "Sample 2: action=13, reward=-1300.0\n",
            "Sample 3: action=30, reward=-1300.0\n",
            "Sample 4: action=53, reward=-1300.0\n",
            "[Episode 710] Reward: -1300.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=33, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 711] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 712] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0.])\n",
            "Sample 0: action=21, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 713] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=39, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=8, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 714] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=17, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=3, reward=0.0\n",
            "[Episode 715] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=39, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 716] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=42, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 717] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=28, reward=0.0\n",
            "Sample 2: action=42, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 718] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=10, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 719] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=19, reward=0.0\n",
            "Sample 1: action=42, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 720] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=19, reward=0.0\n",
            "Sample 1: action=24, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=11, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 721] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 722] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=24, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=40, reward=0.0\n",
            "Sample 4: action=11, reward=0.0\n",
            "[Episode 723] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=8, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 724] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=-1000.0\n",
            "Sample 1: action=22, reward=-1000.0\n",
            "Sample 2: action=16, reward=-1000.0\n",
            "Sample 3: action=30, reward=-1000.0\n",
            "Sample 4: action=11, reward=-1000.0\n",
            "[Episode 725] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=21, reward=0.0\n",
            "Sample 2: action=40, reward=0.0\n",
            "Sample 3: action=32, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 726] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=4, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 727] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=18, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=13, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 728] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=34, reward=0.0\n",
            "[Episode 729] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=13, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=44, reward=0.0\n",
            "[Episode 730] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=21, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=41, reward=0.0\n",
            "[Episode 731] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=34, reward=0.0\n",
            "Sample 1: action=21, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 732] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=2, reward=0.0\n",
            "Sample 1: action=4, reward=0.0\n",
            "Sample 2: action=34, reward=0.0\n",
            "Sample 3: action=25, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 733] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=34, reward=0.0\n",
            "Sample 2: action=44, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 734] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=19, reward=0.0\n",
            "Sample 2: action=39, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 735] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=9, reward=0.0\n",
            "Sample 1: action=0, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=21, reward=0.0\n",
            "[Episode 736] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=19, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=0, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 737] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=28, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=1, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 738] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=2, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=0, reward=0.0\n",
            "Sample 3: action=1, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 739] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=42, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 740] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=2, reward=-1000.0\n",
            "Sample 1: action=53, reward=-1000.0\n",
            "Sample 2: action=32, reward=-1000.0\n",
            "Sample 3: action=1, reward=-1000.0\n",
            "Sample 4: action=16, reward=-1000.0\n",
            "[Episode 741] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=42, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 742] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=9, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 743] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=-1000.0\n",
            "Sample 1: action=31, reward=-1000.0\n",
            "Sample 2: action=32, reward=-1000.0\n",
            "Sample 3: action=8, reward=-1000.0\n",
            "Sample 4: action=25, reward=-1000.0\n",
            "[Episode 744] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=40, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=32, reward=0.0\n",
            "Sample 4: action=33, reward=0.0\n",
            "[Episode 745] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=33, reward=0.0\n",
            "[Episode 746] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 747] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=-1000.0\n",
            "Sample 1: action=53, reward=-1000.0\n",
            "Sample 2: action=32, reward=-1000.0\n",
            "Sample 3: action=8, reward=-1000.0\n",
            "Sample 4: action=8, reward=-1000.0\n",
            "[Episode 748] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=33, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=26, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 749] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=32, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 750] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=27, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=1, reward=0.0\n",
            "[Episode 751] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=37, reward=0.0\n",
            "Sample 2: action=13, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 752] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=39, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 753] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=33, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=15, reward=0.0\n",
            "[Episode 754] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=37, reward=0.0\n",
            "Sample 4: action=3, reward=0.0\n",
            "[Episode 755] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=40, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=3, reward=0.0\n",
            "[Episode 756] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=2, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=39, reward=0.0\n",
            "Sample 4: action=18, reward=0.0\n",
            "[Episode 757] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=40, reward=0.0\n",
            "Sample 2: action=15, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 758] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=39, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 759] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=38, reward=0.0\n",
            "Sample 2: action=3, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=27, reward=0.0\n",
            "[Episode 760] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=27, reward=0.0\n",
            "Sample 1: action=37, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=13, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 761] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=13, reward=0.0\n",
            "Sample 3: action=4, reward=0.0\n",
            "Sample 4: action=26, reward=0.0\n",
            "[Episode 762] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=13, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=3, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 763] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=3, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=5, reward=0.0\n",
            "[Episode 764] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=36, reward=0.0\n",
            "Sample 1: action=17, reward=0.0\n",
            "Sample 2: action=43, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=39, reward=0.0\n",
            "[Episode 765] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=3, reward=0.0\n",
            "Sample 2: action=13, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=24, reward=0.0\n",
            "[Episode 766] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=3, reward=0.0\n",
            "[Episode 767] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=38, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=13, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 768] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=37, reward=0.0\n",
            "Sample 2: action=20, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 769] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=15, reward=3000.0\n",
            "Sample 1: action=29, reward=3000.0\n",
            "Sample 2: action=7, reward=3000.0\n",
            "Sample 3: action=23, reward=3000.0\n",
            "Sample 4: action=12, reward=3000.0\n",
            "[Episode 770] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=15, reward=0.0\n",
            "Sample 1: action=38, reward=0.0\n",
            "Sample 2: action=21, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=11, reward=0.0\n",
            "[Episode 771] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=29, reward=3000.0\n",
            "Sample 1: action=20, reward=3000.0\n",
            "Sample 2: action=2, reward=3000.0\n",
            "Sample 3: action=10, reward=3000.0\n",
            "Sample 4: action=7, reward=3000.0\n",
            "[Episode 772] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=15, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 773] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=29, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 774] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 775] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=21, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 776] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=-1000.0\n",
            "Sample 1: action=35, reward=-1000.0\n",
            "Sample 2: action=2, reward=-1000.0\n",
            "Sample 3: action=8, reward=-1000.0\n",
            "Sample 4: action=1, reward=-1000.0\n",
            "[Episode 777] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 778] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=29, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 779] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 780] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 781] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=35, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=8, reward=0.0\n",
            "Sample 4: action=2, reward=0.0\n",
            "[Episode 782] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=35, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 783] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 784] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=18, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=29, reward=0.0\n",
            "Sample 4: action=18, reward=0.0\n",
            "[Episode 785] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=41, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=18, reward=0.0\n",
            "[Episode 786] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=18, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=44, reward=0.0\n",
            "Sample 4: action=2, reward=0.0\n",
            "[Episode 787] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=42, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 788] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=15, reward=0.0\n",
            "Sample 4: action=18, reward=0.0\n",
            "[Episode 789] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=3000.0\n",
            "Sample 1: action=25, reward=3000.0\n",
            "Sample 2: action=2, reward=3000.0\n",
            "Sample 3: action=29, reward=3000.0\n",
            "Sample 4: action=17, reward=3000.0\n",
            "[Episode 790] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=-1000.0\n",
            "Sample 1: action=4, reward=-1000.0\n",
            "Sample 2: action=53, reward=-1000.0\n",
            "Sample 3: action=14, reward=-1000.0\n",
            "Sample 4: action=53, reward=-1000.0\n",
            "[Episode 791] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 792] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=17, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=21, reward=0.0\n",
            "[Episode 793] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=5, reward=0.0\n",
            "Sample 1: action=38, reward=0.0\n",
            "Sample 2: action=13, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=12, reward=0.0\n",
            "[Episode 794] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=13, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 795] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=2, reward=0.0\n",
            "[Episode 796] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 797] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=42, reward=0.0\n",
            "Sample 2: action=5, reward=0.0\n",
            "Sample 3: action=13, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 798] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=-1000.0\n",
            "Sample 1: action=7, reward=-1000.0\n",
            "Sample 2: action=37, reward=-1000.0\n",
            "Sample 3: action=11, reward=-1000.0\n",
            "Sample 4: action=9, reward=-1000.0\n",
            "[Episode 799] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=0, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=44, reward=0.0\n",
            "Sample 4: action=13, reward=0.0\n",
            "[Episode 800] Reward: 0.0 | Loss: -0.0000\n",
            "‚úì Model saved at ep 800\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=43, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 801] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=0, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=27, reward=0.0\n",
            "[Episode 802] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=44, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=22, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 803] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 804] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=24, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=4, reward=0.0\n",
            "[Episode 805] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=24, reward=0.0\n",
            "Sample 3: action=34, reward=0.0\n",
            "Sample 4: action=1, reward=0.0\n",
            "[Episode 806] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=18, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 807] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 808] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=12, reward=0.0\n",
            "Sample 2: action=44, reward=0.0\n",
            "Sample 3: action=34, reward=0.0\n",
            "Sample 4: action=44, reward=0.0\n",
            "[Episode 809] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=10, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 810] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=39, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 811] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=19, reward=0.0\n",
            "Sample 2: action=10, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 812] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=15, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 813] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=0.0\n",
            "Sample 1: action=24, reward=0.0\n",
            "Sample 2: action=21, reward=0.0\n",
            "Sample 3: action=37, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 814] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=21, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=32, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 815] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=21, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 816] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=18, reward=0.0\n",
            "Sample 2: action=44, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=24, reward=0.0\n",
            "[Episode 817] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=15, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=1, reward=0.0\n",
            "Sample 4: action=21, reward=0.0\n",
            "[Episode 818] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 819] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=4, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 820] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 821] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=27, reward=0.0\n",
            "[Episode 822] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=-1000.0\n",
            "Sample 1: action=43, reward=-1000.0\n",
            "Sample 2: action=6, reward=-1000.0\n",
            "Sample 3: action=10, reward=-1000.0\n",
            "Sample 4: action=19, reward=-1000.0\n",
            "[Episode 823] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=18, reward=0.0\n",
            "Sample 3: action=30, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 824] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 825] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=41, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 826] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 827] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 828] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=9, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=13, reward=0.0\n",
            "Sample 3: action=26, reward=0.0\n",
            "Sample 4: action=33, reward=0.0\n",
            "[Episode 829] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=33, reward=-1000.0\n",
            "Sample 1: action=4, reward=-1000.0\n",
            "Sample 2: action=12, reward=-1000.0\n",
            "Sample 3: action=6, reward=-1000.0\n",
            "Sample 4: action=9, reward=-1000.0\n",
            "[Episode 830] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=-1000.0\n",
            "Sample 1: action=28, reward=-1000.0\n",
            "Sample 2: action=25, reward=-1000.0\n",
            "Sample 3: action=33, reward=-1000.0\n",
            "Sample 4: action=12, reward=-1000.0\n",
            "[Episode 831] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=9, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=20, reward=0.0\n",
            "[Episode 832] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=28, reward=0.0\n",
            "Sample 2: action=38, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=33, reward=0.0\n",
            "[Episode 833] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=28, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=26, reward=0.0\n",
            "Sample 4: action=3, reward=0.0\n",
            "[Episode 834] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 835] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 836] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 837] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=44, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 838] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=0, reward=0.0\n",
            "[Episode 839] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=43, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=22, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 840] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=44, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 841] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=0, reward=0.0\n",
            "[Episode 842] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=33, reward=0.0\n",
            "[Episode 843] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=0.0\n",
            "Sample 1: action=41, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=4, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 844] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=27, reward=0.0\n",
            "Sample 4: action=44, reward=0.0\n",
            "[Episode 845] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=35, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=27, reward=0.0\n",
            "Sample 4: action=4, reward=0.0\n",
            "[Episode 846] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=44, reward=0.0\n",
            "Sample 2: action=25, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=12, reward=0.0\n",
            "[Episode 847] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=4, reward=0.0\n",
            "Sample 3: action=35, reward=0.0\n",
            "Sample 4: action=40, reward=0.0\n",
            "[Episode 848] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=0.0\n",
            "Sample 1: action=43, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=35, reward=0.0\n",
            "Sample 4: action=14, reward=0.0\n",
            "[Episode 849] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=40, reward=0.0\n",
            "Sample 4: action=27, reward=0.0\n",
            "[Episode 850] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=3000.0\n",
            "Sample 1: action=44, reward=3000.0\n",
            "Sample 2: action=5, reward=3000.0\n",
            "Sample 3: action=7, reward=3000.0\n",
            "Sample 4: action=31, reward=3000.0\n",
            "[Episode 851] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=42, reward=0.0\n",
            "Sample 2: action=3, reward=0.0\n",
            "Sample 3: action=40, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 852] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=18, reward=0.0\n",
            "Sample 1: action=26, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=42, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 853] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=18, reward=0.0\n",
            "Sample 2: action=3, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=12, reward=0.0\n",
            "[Episode 854] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=42, reward=0.0\n",
            "Sample 4: action=4, reward=0.0\n",
            "[Episode 855] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=5, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=3, reward=0.0\n",
            "Sample 4: action=4, reward=0.0\n",
            "[Episode 856] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=18, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 857] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=39, reward=0.0\n",
            "Sample 2: action=18, reward=0.0\n",
            "Sample 3: action=3, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 858] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=37, reward=0.0\n",
            "[Episode 859] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=42, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 860] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=14, reward=3000.0\n",
            "Sample 1: action=16, reward=3000.0\n",
            "Sample 2: action=22, reward=3000.0\n",
            "Sample 3: action=43, reward=3000.0\n",
            "Sample 4: action=17, reward=3000.0\n",
            "[Episode 861] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=9, reward=0.0\n",
            "Sample 3: action=36, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 862] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=30, reward=3000.0\n",
            "Sample 1: action=27, reward=3000.0\n",
            "Sample 2: action=16, reward=3000.0\n",
            "Sample 3: action=25, reward=3000.0\n",
            "Sample 4: action=43, reward=3000.0\n",
            "[Episode 863] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=29, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 864] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0.])\n",
            "Sample 0: action=13, reward=3000.0\n",
            "Sample 1: action=53, reward=3000.0\n",
            "Sample 2: action=16, reward=3000.0\n",
            "Sample 3: action=32, reward=3000.0\n",
            "Sample 4: action=17, reward=3000.0\n",
            "[Episode 865] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=2, reward=0.0\n",
            "[Episode 866] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=9, reward=0.0\n",
            "Sample 2: action=42, reward=0.0\n",
            "Sample 3: action=30, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 867] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=18, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=10, reward=0.0\n",
            "Sample 3: action=27, reward=0.0\n",
            "Sample 4: action=36, reward=0.0\n",
            "[Episode 868] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=24, reward=0.0\n",
            "Sample 2: action=39, reward=0.0\n",
            "Sample 3: action=3, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 869] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 870] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=3, reward=0.0\n",
            "Sample 3: action=11, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 871] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=3, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=24, reward=0.0\n",
            "Sample 3: action=12, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 872] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=3, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=11, reward=0.0\n",
            "Sample 4: action=21, reward=0.0\n",
            "[Episode 873] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=28, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=24, reward=0.0\n",
            "[Episode 874] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=22, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 875] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=28, reward=0.0\n",
            "Sample 2: action=3, reward=0.0\n",
            "Sample 3: action=40, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 876] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=-1000.0\n",
            "Sample 1: action=30, reward=-1000.0\n",
            "Sample 2: action=33, reward=-1000.0\n",
            "Sample 3: action=37, reward=-1000.0\n",
            "Sample 4: action=19, reward=-1000.0\n",
            "[Episode 877] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=43, reward=0.0\n",
            "Sample 2: action=1, reward=0.0\n",
            "Sample 3: action=37, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 878] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=3000.0\n",
            "Sample 1: action=42, reward=3000.0\n",
            "Sample 2: action=27, reward=3000.0\n",
            "Sample 3: action=33, reward=3000.0\n",
            "Sample 4: action=30, reward=3000.0\n",
            "[Episode 879] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=42, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=4, reward=0.0\n",
            "[Episode 880] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=27, reward=0.0\n",
            "Sample 2: action=24, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=18, reward=0.0\n",
            "[Episode 881] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=42, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=13, reward=0.0\n",
            "[Episode 882] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=35, reward=0.0\n",
            "[Episode 883] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=18, reward=0.0\n",
            "Sample 1: action=44, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 884] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=34, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=30, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 885] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-3900., -3900., -3900., -3900., -3900., -3900., -3900., -3900., -3900.,\n",
            "        -3900., -3900., -3900., -3900., -3900., -3900., -3900.])\n",
            "Baseline: -3900.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=9, reward=-3900.0\n",
            "Sample 1: action=30, reward=-3900.0\n",
            "Sample 2: action=8, reward=-3900.0\n",
            "Sample 3: action=40, reward=-3900.0\n",
            "Sample 4: action=21, reward=-3900.0\n",
            "[Episode 886] Reward: -3900.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=21, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=37, reward=0.0\n",
            "[Episode 887] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=39, reward=0.0\n",
            "Sample 2: action=34, reward=0.0\n",
            "Sample 3: action=8, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 888] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=31, reward=-1000.0\n",
            "Sample 1: action=11, reward=-1000.0\n",
            "Sample 2: action=39, reward=-1000.0\n",
            "Sample 3: action=34, reward=-1000.0\n",
            "Sample 4: action=26, reward=-1000.0\n",
            "[Episode 889] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=3, reward=0.0\n",
            "Sample 4: action=38, reward=0.0\n",
            "[Episode 890] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=12, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 891] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=28, reward=0.0\n",
            "Sample 2: action=18, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=37, reward=0.0\n",
            "[Episode 892] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=40, reward=0.0\n",
            "[Episode 893] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=42, reward=0.0\n",
            "Sample 3: action=25, reward=0.0\n",
            "Sample 4: action=11, reward=0.0\n",
            "[Episode 894] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=28, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=31, reward=0.0\n",
            "Sample 3: action=18, reward=0.0\n",
            "Sample 4: action=39, reward=0.0\n",
            "[Episode 895] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=19, reward=0.0\n",
            "Sample 1: action=27, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=15, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 896] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=21, reward=0.0\n",
            "Sample 1: action=44, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 897] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=41, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=30, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 898] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=26, reward=0.0\n",
            "Sample 2: action=15, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 899] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=1, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 900] Reward: 0.0 | Loss: -0.0000\n",
            "‚úì Model saved at ep 900\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=11, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 901] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=14, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=11, reward=0.0\n",
            "Sample 4: action=21, reward=0.0\n",
            "[Episode 902] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=20, reward=0.0\n",
            "Sample 1: action=13, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=25, reward=0.0\n",
            "[Episode 903] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=33, reward=-1000.0\n",
            "Sample 1: action=14, reward=-1000.0\n",
            "Sample 2: action=4, reward=-1000.0\n",
            "Sample 3: action=11, reward=-1000.0\n",
            "Sample 4: action=20, reward=-1000.0\n",
            "[Episode 904] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=13, reward=0.0\n",
            "Sample 1: action=43, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=10, reward=0.0\n",
            "[Episode 905] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=4, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=14, reward=0.0\n",
            "Sample 4: action=28, reward=0.0\n",
            "[Episode 906] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=20, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=38, reward=0.0\n",
            "[Episode 907] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=40, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=20, reward=0.0\n",
            "[Episode 908] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=11, reward=0.0\n",
            "Sample 3: action=4, reward=0.0\n",
            "Sample 4: action=20, reward=0.0\n",
            "[Episode 909] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=17, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=27, reward=0.0\n",
            "[Episode 910] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=21, reward=0.0\n",
            "Sample 1: action=0, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=44, reward=0.0\n",
            "Sample 4: action=33, reward=0.0\n",
            "[Episode 911] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=32, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=5, reward=0.0\n",
            "[Episode 912] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=27, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=21, reward=0.0\n",
            "Sample 3: action=29, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 913] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=21, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=8, reward=0.0\n",
            "[Episode 914] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=5, reward=0.0\n",
            "Sample 3: action=8, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 915] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=-1000.0\n",
            "Sample 1: action=8, reward=-1000.0\n",
            "Sample 2: action=27, reward=-1000.0\n",
            "Sample 3: action=37, reward=-1000.0\n",
            "Sample 4: action=21, reward=-1000.0\n",
            "[Episode 916] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=-1000.0\n",
            "Sample 1: action=8, reward=-1000.0\n",
            "Sample 2: action=43, reward=-1000.0\n",
            "Sample 3: action=7, reward=-1000.0\n",
            "Sample 4: action=6, reward=-1000.0\n",
            "[Episode 917] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=37, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=21, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 918] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=12, reward=0.0\n",
            "Sample 4: action=23, reward=0.0\n",
            "[Episode 919] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=21, reward=0.0\n",
            "Sample 1: action=37, reward=0.0\n",
            "Sample 2: action=1, reward=0.0\n",
            "Sample 3: action=1, reward=0.0\n",
            "Sample 4: action=24, reward=0.0\n",
            "[Episode 920] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=31, reward=0.0\n",
            "Sample 2: action=40, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 921] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=-1000.0\n",
            "Sample 1: action=1, reward=-1000.0\n",
            "Sample 2: action=26, reward=-1000.0\n",
            "Sample 3: action=23, reward=-1000.0\n",
            "Sample 4: action=24, reward=-1000.0\n",
            "[Episode 922] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=21, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=1, reward=0.0\n",
            "[Episode 923] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=41, reward=0.0\n",
            "[Episode 924] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=20, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=21, reward=0.0\n",
            "[Episode 925] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=26, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=42, reward=0.0\n",
            "Sample 4: action=24, reward=0.0\n",
            "[Episode 926] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=-1000.0\n",
            "Sample 1: action=18, reward=-1000.0\n",
            "Sample 2: action=53, reward=-1000.0\n",
            "Sample 3: action=6, reward=-1000.0\n",
            "Sample 4: action=30, reward=-1000.0\n",
            "[Episode 927] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 928] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "[Episode 929] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=24, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=22, reward=0.0\n",
            "Sample 4: action=2, reward=0.0\n",
            "[Episode 930] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=14, reward=0.0\n",
            "Sample 1: action=12, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 931] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=22, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=24, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 932] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=18, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=22, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 933] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=30, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=19, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 934] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=18, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=1, reward=0.0\n",
            "[Episode 935] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=1, reward=0.0\n",
            "Sample 2: action=30, reward=0.0\n",
            "Sample 3: action=12, reward=0.0\n",
            "Sample 4: action=13, reward=0.0\n",
            "[Episode 936] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=13, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 937] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=12, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 938] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=23, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=13, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 939] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=42, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=40, reward=0.0\n",
            "[Episode 940] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-700., -700., -700., -700., -700., -700., -700., -700., -700., -700.,\n",
            "        -700., -700., -700., -700., -700., -700., -700.])\n",
            "Baseline: -700.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=-700.0\n",
            "Sample 1: action=1, reward=-700.0\n",
            "Sample 2: action=0, reward=-700.0\n",
            "Sample 3: action=30, reward=-700.0\n",
            "Sample 4: action=53, reward=-700.0\n",
            "[Episode 941] Reward: -700.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=33, reward=0.0\n",
            "Sample 2: action=0, reward=0.0\n",
            "Sample 3: action=27, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 942] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=23, reward=0.0\n",
            "Sample 3: action=3, reward=0.0\n",
            "Sample 4: action=30, reward=0.0\n",
            "[Episode 943] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=33, reward=0.0\n",
            "Sample 2: action=15, reward=0.0\n",
            "Sample 3: action=43, reward=0.0\n",
            "Sample 4: action=2, reward=0.0\n",
            "[Episode 944] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=33, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=36, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 945] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=17, reward=0.0\n",
            "Sample 2: action=44, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 946] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=15, reward=0.0\n",
            "Sample 1: action=40, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=36, reward=0.0\n",
            "[Episode 947] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=36, reward=0.0\n",
            "Sample 4: action=2, reward=0.0\n",
            "[Episode 948] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=41, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=43, reward=0.0\n",
            "[Episode 949] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=31, reward=0.0\n",
            "Sample 1: action=40, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=6, reward=0.0\n",
            "Sample 4: action=33, reward=0.0\n",
            "[Episode 950] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=31, reward=0.0\n",
            "Sample 4: action=15, reward=0.0\n",
            "[Episode 951] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=3, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 952] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=5, reward=0.0\n",
            "Sample 3: action=25, reward=0.0\n",
            "Sample 4: action=19, reward=0.0\n",
            "[Episode 953] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=-1000.0\n",
            "Sample 1: action=20, reward=-1000.0\n",
            "Sample 2: action=37, reward=-1000.0\n",
            "Sample 3: action=16, reward=-1000.0\n",
            "Sample 4: action=32, reward=-1000.0\n",
            "[Episode 954] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=28, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=38, reward=0.0\n",
            "Sample 3: action=25, reward=0.0\n",
            "Sample 4: action=14, reward=0.0\n",
            "[Episode 955] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=3000.0\n",
            "Sample 1: action=14, reward=3000.0\n",
            "Sample 2: action=4, reward=3000.0\n",
            "Sample 3: action=42, reward=3000.0\n",
            "Sample 4: action=29, reward=3000.0\n",
            "[Episode 956] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=4, reward=0.0\n",
            "Sample 2: action=14, reward=0.0\n",
            "Sample 3: action=40, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 957] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=28, reward=0.0\n",
            "Sample 1: action=40, reward=0.0\n",
            "Sample 2: action=3, reward=0.0\n",
            "Sample 3: action=38, reward=0.0\n",
            "Sample 4: action=26, reward=0.0\n",
            "[Episode 958] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=0.0\n",
            "Sample 1: action=38, reward=0.0\n",
            "Sample 2: action=3, reward=0.0\n",
            "Sample 3: action=5, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 959] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=37, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 960] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=11, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 961] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=6, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 962] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=27, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=29, reward=0.0\n",
            "Sample 3: action=22, reward=0.0\n",
            "Sample 4: action=32, reward=0.0\n",
            "[Episode 963] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=13, reward=0.0\n",
            "Sample 4: action=11, reward=0.0\n",
            "[Episode 964] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=6, reward=0.0\n",
            "Sample 2: action=13, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 965] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=-1000.0\n",
            "Sample 1: action=27, reward=-1000.0\n",
            "Sample 2: action=40, reward=-1000.0\n",
            "Sample 3: action=33, reward=-1000.0\n",
            "Sample 4: action=17, reward=-1000.0\n",
            "[Episode 966] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=36, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=37, reward=0.0\n",
            "[Episode 967] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=11, reward=0.0\n",
            "Sample 1: action=30, reward=0.0\n",
            "Sample 2: action=22, reward=0.0\n",
            "Sample 3: action=27, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 968] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=3, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=43, reward=0.0\n",
            "Sample 3: action=10, reward=0.0\n",
            "Sample 4: action=29, reward=0.0\n",
            "[Episode 969] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=-1000.0\n",
            "Sample 1: action=0, reward=-1000.0\n",
            "Sample 2: action=26, reward=-1000.0\n",
            "Sample 3: action=42, reward=-1000.0\n",
            "Sample 4: action=8, reward=-1000.0\n",
            "[Episode 970] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=8, reward=-1000.0\n",
            "Sample 1: action=44, reward=-1000.0\n",
            "Sample 2: action=3, reward=-1000.0\n",
            "Sample 3: action=5, reward=-1000.0\n",
            "Sample 4: action=53, reward=-1000.0\n",
            "[Episode 971] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=32, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 972] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=8, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=16, reward=0.0\n",
            "Sample 3: action=44, reward=0.0\n",
            "Sample 4: action=27, reward=0.0\n",
            "[Episode 973] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=6, reward=0.0\n",
            "Sample 1: action=27, reward=0.0\n",
            "Sample 2: action=5, reward=0.0\n",
            "Sample 3: action=24, reward=0.0\n",
            "Sample 4: action=0, reward=0.0\n",
            "[Episode 974] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=5, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 975] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=44, reward=0.0\n",
            "Sample 2: action=27, reward=0.0\n",
            "Sample 3: action=8, reward=0.0\n",
            "Sample 4: action=0, reward=0.0\n",
            "[Episode 976] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=7, reward=-1000.0\n",
            "Sample 1: action=1, reward=-1000.0\n",
            "Sample 2: action=43, reward=-1000.0\n",
            "Sample 3: action=2, reward=-1000.0\n",
            "Sample 4: action=5, reward=-1000.0\n",
            "[Episode 977] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=-1000.0\n",
            "Sample 1: action=20, reward=-1000.0\n",
            "Sample 2: action=12, reward=-1000.0\n",
            "Sample 3: action=11, reward=-1000.0\n",
            "Sample 4: action=5, reward=-1000.0\n",
            "[Episode 978] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=12, reward=0.0\n",
            "Sample 1: action=2, reward=0.0\n",
            "Sample 2: action=5, reward=0.0\n",
            "Sample 3: action=17, reward=0.0\n",
            "Sample 4: action=6, reward=0.0\n",
            "[Episode 979] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=32, reward=-1000.0\n",
            "Sample 1: action=8, reward=-1000.0\n",
            "Sample 2: action=7, reward=-1000.0\n",
            "Sample 3: action=15, reward=-1000.0\n",
            "Sample 4: action=5, reward=-1000.0\n",
            "[Episode 980] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=20, reward=0.0\n",
            "Sample 1: action=8, reward=0.0\n",
            "Sample 2: action=2, reward=0.0\n",
            "Sample 3: action=0, reward=0.0\n",
            "Sample 4: action=22, reward=0.0\n",
            "[Episode 981] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=1, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=7, reward=0.0\n",
            "Sample 4: action=37, reward=0.0\n",
            "[Episode 982] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=15, reward=0.0\n",
            "Sample 2: action=32, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=31, reward=0.0\n",
            "[Episode 983] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=20, reward=0.0\n",
            "Sample 1: action=7, reward=0.0\n",
            "Sample 2: action=38, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 984] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=29, reward=0.0\n",
            "Sample 1: action=19, reward=0.0\n",
            "Sample 2: action=53, reward=0.0\n",
            "Sample 3: action=20, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 985] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=17, reward=0.0\n",
            "Sample 1: action=29, reward=0.0\n",
            "Sample 2: action=10, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=7, reward=0.0\n",
            "[Episode 986] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=3, reward=0.0\n",
            "Sample 1: action=16, reward=0.0\n",
            "Sample 2: action=18, reward=0.0\n",
            "Sample 3: action=9, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 987] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000., -1000.,\n",
            "        -1000., -1000., -1000., -1000., -1000.])\n",
            "Baseline: -1000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=-1000.0\n",
            "Sample 1: action=38, reward=-1000.0\n",
            "Sample 2: action=1, reward=-1000.0\n",
            "Sample 3: action=29, reward=-1000.0\n",
            "Sample 4: action=41, reward=-1000.0\n",
            "[Episode 988] Reward: -1000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=16, reward=0.0\n",
            "Sample 1: action=27, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=33, reward=0.0\n",
            "Sample 4: action=53, reward=0.0\n",
            "[Episode 989] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=23, reward=0.0\n",
            "Sample 1: action=0, reward=0.0\n",
            "Sample 2: action=28, reward=0.0\n",
            "Sample 3: action=41, reward=0.0\n",
            "Sample 4: action=36, reward=0.0\n",
            "[Episode 990] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=28, reward=0.0\n",
            "Sample 1: action=22, reward=0.0\n",
            "Sample 2: action=7, reward=0.0\n",
            "Sample 3: action=23, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 991] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=19, reward=0.0\n",
            "Sample 1: action=40, reward=0.0\n",
            "Sample 2: action=24, reward=0.0\n",
            "Sample 3: action=3, reward=0.0\n",
            "Sample 4: action=40, reward=0.0\n",
            "[Episode 992] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=24, reward=0.0\n",
            "Sample 1: action=10, reward=0.0\n",
            "Sample 2: action=8, reward=0.0\n",
            "Sample 3: action=16, reward=0.0\n",
            "Sample 4: action=27, reward=0.0\n",
            "[Episode 993] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=10, reward=0.0\n",
            "Sample 1: action=3, reward=0.0\n",
            "Sample 2: action=19, reward=0.0\n",
            "Sample 3: action=28, reward=0.0\n",
            "Sample 4: action=16, reward=0.0\n",
            "[Episode 994] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([-1500., -1500., -1500., -1500., -1500., -1500., -1500., -1500., -1500.,\n",
            "        -1500., -1500., -1500., -1500., -1500., -1500., -1500., -1500., -1500.,\n",
            "        -1500., -1500., -1500., -1500., -1500., -1500., -1500.])\n",
            "Baseline: -1500.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=33, reward=-1500.0\n",
            "Sample 1: action=44, reward=-1500.0\n",
            "Sample 2: action=17, reward=-1500.0\n",
            "Sample 3: action=25, reward=-1500.0\n",
            "Sample 4: action=18, reward=-1500.0\n",
            "[Episode 995] Reward: -1500.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=26, reward=0.0\n",
            "Sample 1: action=5, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=12, reward=0.0\n",
            "Sample 4: action=44, reward=0.0\n",
            "[Episode 996] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=25, reward=3000.0\n",
            "Sample 1: action=33, reward=3000.0\n",
            "Sample 2: action=43, reward=3000.0\n",
            "Sample 3: action=19, reward=3000.0\n",
            "Sample 4: action=0, reward=3000.0\n",
            "[Episode 997] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "Sample 0: action=0, reward=0.0\n",
            "Sample 1: action=53, reward=0.0\n",
            "Sample 2: action=33, reward=0.0\n",
            "Sample 3: action=53, reward=0.0\n",
            "Sample 4: action=17, reward=0.0\n",
            "[Episode 998] Reward: 0.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000., 3000.,\n",
            "        3000., 3000., 3000., 3000., 3000., 3000.])\n",
            "Baseline: 3000.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0.])\n",
            "Sample 0: action=25, reward=3000.0\n",
            "Sample 1: action=33, reward=3000.0\n",
            "Sample 2: action=2, reward=3000.0\n",
            "Sample 3: action=11, reward=3000.0\n",
            "Sample 4: action=44, reward=3000.0\n",
            "[Episode 999] Reward: 3000.0 | Loss: -0.0000\n",
            "\n",
            "üîç DEBUG TRAINING BATCH:\n",
            "Rewards: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Baseline: 0.0\n",
            "Advantages: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0.])\n",
            "Sample 0: action=33, reward=0.0\n",
            "Sample 1: action=25, reward=0.0\n",
            "Sample 2: action=26, reward=0.0\n",
            "Sample 3: action=2, reward=0.0\n",
            "Sample 4: action=14, reward=0.0\n",
            "[Episode 1000] Reward: 0.0 | Loss: -0.0000\n",
            "‚úì Model saved at ep 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pj8pM2hGiN5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXpNDlIH0_Jr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}